{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if summarys_entidades_nomeadas.csv does not exist, create it\n",
    "if not os.path.exists('summarys_entidades_nomeadas.csv'):\n",
    "    import pymongo, certifi\n",
    "    ca = certifi.where()\n",
    "    db = pymongo.MongoClient(\"mongodb+srv://nlp:nlphuehue@manga.9pvhp.mongodb.net/manga?retryWrites=true&w=majority\", tlsCAFile=ca)\n",
    "    manga = db.nlp.manga # {\"_id\": |id, \"title\": title, \"summary\": manga summary, \"keywords\": {keyword: tf, keyword: tf, keyword: tf}}\n",
    "    summarys_ = manga.find({}, {\"_id\": 0, \"summary\": 1})\n",
    "    summarys = [x[\"summary\"] for x in summarys_]\n",
    "    summarys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    30\n",
       "negative    20\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entidades_nomeadas = ['Demon', 'King', 'Emperor', 'Duke', 'Prince', 'Lin', 'High', 'God', 'Ye', 'Crown', 'Great', 'Dragon',\n",
    "                      'Princess', 'Xiao', 'Original', 'Han', 'Li', 'Yu', 'Bai', 'Kang']\n",
    "\n",
    "if not os.path.exists('summarys_entidades_nomeadas.csv') and not os.path.exists('summarys_entidades_nomeadas_n_classficadas.csv'):\n",
    "    summarys_entidades_nomeadas = {}\n",
    "    summarys_entidades_nomeadas_n_classficadas = {}\n",
    "    quant_manual_classificados = 50\n",
    "    id_ = 0\n",
    "    for summary in summarys:\n",
    "        if any(entidade in summary for entidade in entidades_nomeadas):\n",
    "            if id_ < quant_manual_classificados:\n",
    "                id_ += 1\n",
    "                # summarys_entidades_nomeadas[id_] = {}\n",
    "                # summarys_entidades_nomeadas[id_][\"summary\"] = summary\n",
    "                # if input(\"{}\\n\".format(summary)) == \"s\":\n",
    "                #     summarys_entidades_nomeadas[id_][\"sentiment\"] = \"positive\"\n",
    "                # else:\n",
    "                #     summarys_entidades_nomeadas[id_][\"sentiment\"] = \"negative\"\n",
    "                # summarys_entidades_nomeadas[id_][\"entidades\"] = []\n",
    "                # for entidade in entidades_nomeadas:\n",
    "                #     if entidade in summary:\n",
    "                #         summarys_entidades_nomeadas[id_][\"entidades\"].append(entidade)\n",
    "            else:\n",
    "                summarys_entidades_nomeadas_n_classficadas[id_] = {}\n",
    "                summarys_entidades_nomeadas_n_classficadas[id_][\"summary\"] = summary\n",
    "                summarys_entidades_nomeadas_n_classficadas[id_][\"entidades\"] = []\n",
    "                for entidade in entidades_nomeadas:\n",
    "                    if entidade in summary:\n",
    "                        summarys_entidades_nomeadas_n_classficadas[id_][\"entidades\"].append(entidade)\n",
    "\n",
    "\n",
    "    # df = pd.DataFrame.from_dict(summarys_entidades_nomeadas, orient='index')\n",
    "    # df.to_csv(\"summarys_entidades_nomeadas.csv\")\n",
    "\n",
    "    df = pd.DataFrame.from_dict(summarys_entidades_nomeadas_n_classficadas, orient='index')\n",
    "    df.to_csv(\"summarys_entidades_nomeadas_n_classficadas.csv\")\n",
    "\n",
    "df = pd.read_csv(\"summarys_entidades_nomeadas.csv\")\n",
    "df[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([list(df['sentiment'])]).T\n",
    "ohe = OneHotEncoder()\n",
    "y = ohe.fit_transform(labels).toarray()\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "sequences = tokenizer.texts_to_sequences(reviews)\n",
    "padded = pad_sequences(sequences, maxlen=200)\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded, y, test_size=0.2)\n",
    "\n",
    "\n",
    "def rede_neural_classificar_por_palavra(input_dims, n_dims_out):\n",
    "    input_layer = Input(shape=(input_dims,))\n",
    "    x = input_layer\n",
    "    x = Embedding(1000, n_dims_out, name=\"projecao\")(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    y = Dense(2, activation=\"sigmoid\", name=\"classificador\")(x)\n",
    "    return Model(input_layer, y)\n",
    "\n",
    "\n",
    "rede_neural = rede_neural_classificar_por_palavra(200, 2)\n",
    "rede_neural.compile(optimizer=\"adam\", loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=1, patience=20, restore_best_weights=True)\n",
    "model = rede_neural.fit(X_train, y_train, epochs=1000, validation_split=0.2, callbacks=es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classificadas_df = pd.read_csv(\"summarys_entidades_nomeadas_n_classficadas.csv\")\n",
    "n_classificadas_df[\"sentiment\"] = model.predict(n_classificadas_df[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_with_entity = anime_reviews_df[anime_reviews_df[\"entidades\"].str.contains(entity)]\n",
    "\n",
    "positive_reviews_with_entity_unigram = calculate_ngrams_count(reviews_with_entity[reviews_with_entity[\"sentiment\"] == \"positive\"][\"text\"], (1, 1), STOPWORDS_EN)\n",
    "negative_reviews_with_entity_unigram = calculate_ngrams_count(reviews_with_entity[reviews_with_entity[\"sentiment\"] == \"negative\"][\"text\"], (1, 1), STOPWORDS_EN)\n",
    "words = list(set(list(positive_reviews_with_entity_unigram[:n_words][\"ngram\"]) + list(negative_reviews_with_entity_unigram[:n_words][\"ngram\"])))\n",
    "\n",
    "v = rede_neural.get_layer(\"projecao\").get_weights()[0]\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(v[:, 0], v[:, 1], s=1, alpha=0.3, c=\"b\")\n",
    "for s in words:\n",
    "    _n = tokenizer.texts_to_sequences([[s]])[0][0]\n",
    "    plt.text(v[_n, 0], v[_n, 1], s, ha=\"center\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
